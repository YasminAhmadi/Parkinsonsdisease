{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189b4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import  matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237e2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "df=pd.read_csv('pd_speech_features.csv',index_col=0, delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf80959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85247</td>\n",
       "      <td>0.71826</td>\n",
       "      <td>0.57227</td>\n",
       "      <td>240</td>\n",
       "      <td>239</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00218</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5620</td>\n",
       "      <td>2.6445</td>\n",
       "      <td>3.8686</td>\n",
       "      <td>4.2105</td>\n",
       "      <td>5.1221</td>\n",
       "      <td>4.4625</td>\n",
       "      <td>2.6202</td>\n",
       "      <td>3.0004</td>\n",
       "      <td>18.9405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.76686</td>\n",
       "      <td>0.69481</td>\n",
       "      <td>0.53966</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5589</td>\n",
       "      <td>3.6107</td>\n",
       "      <td>23.5155</td>\n",
       "      <td>14.1962</td>\n",
       "      <td>11.0261</td>\n",
       "      <td>9.5082</td>\n",
       "      <td>6.5245</td>\n",
       "      <td>6.3431</td>\n",
       "      <td>45.1780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85083</td>\n",
       "      <td>0.67604</td>\n",
       "      <td>0.58982</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5643</td>\n",
       "      <td>2.3308</td>\n",
       "      <td>9.4959</td>\n",
       "      <td>10.7458</td>\n",
       "      <td>11.0177</td>\n",
       "      <td>4.8066</td>\n",
       "      <td>2.9199</td>\n",
       "      <td>3.1495</td>\n",
       "      <td>4.7666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.41121</td>\n",
       "      <td>0.79672</td>\n",
       "      <td>0.59257</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7805</td>\n",
       "      <td>3.5664</td>\n",
       "      <td>5.2558</td>\n",
       "      <td>14.0403</td>\n",
       "      <td>4.2235</td>\n",
       "      <td>4.6857</td>\n",
       "      <td>4.8460</td>\n",
       "      <td>6.2650</td>\n",
       "      <td>4.0603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.32790</td>\n",
       "      <td>0.79782</td>\n",
       "      <td>0.53028</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1727</td>\n",
       "      <td>5.8416</td>\n",
       "      <td>6.0805</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>7.7817</td>\n",
       "      <td>11.6891</td>\n",
       "      <td>8.2103</td>\n",
       "      <td>5.0559</td>\n",
       "      <td>6.1164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>0.80903</td>\n",
       "      <td>0.56355</td>\n",
       "      <td>0.28385</td>\n",
       "      <td>417</td>\n",
       "      <td>416</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0706</td>\n",
       "      <td>3.0190</td>\n",
       "      <td>3.1212</td>\n",
       "      <td>2.4921</td>\n",
       "      <td>3.5844</td>\n",
       "      <td>3.5400</td>\n",
       "      <td>3.3805</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>6.8671</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0</td>\n",
       "      <td>0.16084</td>\n",
       "      <td>0.56499</td>\n",
       "      <td>0.59194</td>\n",
       "      <td>415</td>\n",
       "      <td>413</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9704</td>\n",
       "      <td>1.7451</td>\n",
       "      <td>1.8277</td>\n",
       "      <td>2.4976</td>\n",
       "      <td>5.2981</td>\n",
       "      <td>4.2616</td>\n",
       "      <td>6.3042</td>\n",
       "      <td>10.9058</td>\n",
       "      <td>28.4170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>0.88389</td>\n",
       "      <td>0.72335</td>\n",
       "      <td>0.46815</td>\n",
       "      <td>381</td>\n",
       "      <td>380</td>\n",
       "      <td>0.005069</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>51.5607</td>\n",
       "      <td>44.4641</td>\n",
       "      <td>26.1586</td>\n",
       "      <td>6.3076</td>\n",
       "      <td>2.8601</td>\n",
       "      <td>2.5361</td>\n",
       "      <td>3.5377</td>\n",
       "      <td>3.3545</td>\n",
       "      <td>5.0424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83782</td>\n",
       "      <td>0.74890</td>\n",
       "      <td>0.49823</td>\n",
       "      <td>340</td>\n",
       "      <td>339</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.00092</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>19.1607</td>\n",
       "      <td>12.8312</td>\n",
       "      <td>8.9434</td>\n",
       "      <td>2.2044</td>\n",
       "      <td>1.9496</td>\n",
       "      <td>1.9664</td>\n",
       "      <td>2.6801</td>\n",
       "      <td>2.8332</td>\n",
       "      <td>3.7131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0</td>\n",
       "      <td>0.81304</td>\n",
       "      <td>0.76471</td>\n",
       "      <td>0.46374</td>\n",
       "      <td>340</td>\n",
       "      <td>339</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>62.9927</td>\n",
       "      <td>21.8152</td>\n",
       "      <td>9.2457</td>\n",
       "      <td>4.8555</td>\n",
       "      <td>3.0551</td>\n",
       "      <td>3.0415</td>\n",
       "      <td>4.0116</td>\n",
       "      <td>2.6217</td>\n",
       "      <td>3.1527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "id                                                                    \n",
       "0         1  0.85247  0.71826  0.57227        240               239   \n",
       "0         1  0.76686  0.69481  0.53966        234               233   \n",
       "0         1  0.85083  0.67604  0.58982        232               231   \n",
       "1         0  0.41121  0.79672  0.59257        178               177   \n",
       "1         0  0.32790  0.79782  0.53028        236               235   \n",
       "..      ...      ...      ...      ...        ...               ...   \n",
       "250       0  0.80903  0.56355  0.28385        417               416   \n",
       "250       0  0.16084  0.56499  0.59194        415               413   \n",
       "251       0  0.88389  0.72335  0.46815        381               380   \n",
       "251       0  0.83782  0.74890  0.49823        340               339   \n",
       "251       0  0.81304  0.76471  0.46374        340               339   \n",
       "\n",
       "     meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  ...  \\\n",
       "id                                                                     ...   \n",
       "0            0.008064            0.000087       0.00218      0.000018  ...   \n",
       "0            0.008258            0.000073       0.00195      0.000016  ...   \n",
       "0            0.008340            0.000060       0.00176      0.000015  ...   \n",
       "1            0.010858            0.000183       0.00419      0.000046  ...   \n",
       "1            0.008162            0.002669       0.00535      0.000044  ...   \n",
       "..                ...                 ...           ...           ...  ...   \n",
       "250          0.004627            0.000052       0.00064      0.000003  ...   \n",
       "250          0.004550            0.000220       0.00143      0.000006  ...   \n",
       "251          0.005069            0.000103       0.00076      0.000004  ...   \n",
       "251          0.005679            0.000055       0.00092      0.000005  ...   \n",
       "251          0.005676            0.000037       0.00078      0.000004  ...   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "id                                                          \n",
       "0                       1.5620                     2.6445   \n",
       "0                       1.5589                     3.6107   \n",
       "0                       1.5643                     2.3308   \n",
       "1                       3.7805                     3.5664   \n",
       "1                       6.1727                     5.8416   \n",
       "..                         ...                        ...   \n",
       "250                     3.0706                     3.0190   \n",
       "250                     1.9704                     1.7451   \n",
       "251                    51.5607                    44.4641   \n",
       "251                    19.1607                    12.8312   \n",
       "251                    62.9927                    21.8152   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "id                                                          \n",
       "0                       3.8686                     4.2105   \n",
       "0                      23.5155                    14.1962   \n",
       "0                       9.4959                    10.7458   \n",
       "1                       5.2558                    14.0403   \n",
       "1                       6.0805                     5.7621   \n",
       "..                         ...                        ...   \n",
       "250                     3.1212                     2.4921   \n",
       "250                     1.8277                     2.4976   \n",
       "251                    26.1586                     6.3076   \n",
       "251                     8.9434                     2.2044   \n",
       "251                     9.2457                     4.8555   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "id                                                          \n",
       "0                       5.1221                     4.4625   \n",
       "0                      11.0261                     9.5082   \n",
       "0                      11.0177                     4.8066   \n",
       "1                       4.2235                     4.6857   \n",
       "1                       7.7817                    11.6891   \n",
       "..                         ...                        ...   \n",
       "250                     3.5844                     3.5400   \n",
       "250                     5.2981                     4.2616   \n",
       "251                     2.8601                     2.5361   \n",
       "251                     1.9496                     1.9664   \n",
       "251                     3.0551                     3.0415   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "id                                                          \n",
       "0                       2.6202                     3.0004   \n",
       "0                       6.5245                     6.3431   \n",
       "0                       2.9199                     3.1495   \n",
       "1                       4.8460                     6.2650   \n",
       "1                       8.2103                     5.0559   \n",
       "..                         ...                        ...   \n",
       "250                     3.3805                     3.2003   \n",
       "250                     6.3042                    10.9058   \n",
       "251                     3.5377                     3.3545   \n",
       "251                     2.6801                     2.8332   \n",
       "251                     4.0116                     2.6217   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_36  class  \n",
       "id                                     \n",
       "0                      18.9405      1  \n",
       "0                      45.1780      1  \n",
       "0                       4.7666      1  \n",
       "1                       4.0603      1  \n",
       "1                       6.1164      1  \n",
       "..                         ...    ...  \n",
       "250                     6.8671      0  \n",
       "250                    28.4170      0  \n",
       "251                     5.0424      0  \n",
       "251                     3.7131      0  \n",
       "251                     3.1527      0  \n",
       "\n",
       "[756 rows x 754 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62504b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate columns\n",
    "df = df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f580f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18388e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                       0\n",
       "PPE                          0\n",
       "DFA                          0\n",
       "RPDE                         0\n",
       "numPulses                    0\n",
       "                            ..\n",
       "tqwt_kurtosisValue_dec_33    0\n",
       "tqwt_kurtosisValue_dec_34    0\n",
       "tqwt_kurtosisValue_dec_35    0\n",
       "tqwt_kurtosisValue_dec_36    0\n",
       "class                        0\n",
       "Length: 754, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: sum(x.isnull()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16a1de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.85374</td>\n",
       "      <td>0.76597</td>\n",
       "      <td>0.39811</td>\n",
       "      <td>416</td>\n",
       "      <td>415</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.00231</td>\n",
       "      <td>1.070000e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>10.1883</td>\n",
       "      <td>6.7917</td>\n",
       "      <td>3.0096</td>\n",
       "      <td>2.3981</td>\n",
       "      <td>2.9229</td>\n",
       "      <td>4.1730</td>\n",
       "      <td>4.5809</td>\n",
       "      <td>2.8216</td>\n",
       "      <td>2.7978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.83688</td>\n",
       "      <td>0.59464</td>\n",
       "      <td>0.57059</td>\n",
       "      <td>313</td>\n",
       "      <td>312</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.00171</td>\n",
       "      <td>1.060000e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0107</td>\n",
       "      <td>16.7081</td>\n",
       "      <td>4.5445</td>\n",
       "      <td>2.6795</td>\n",
       "      <td>2.7662</td>\n",
       "      <td>2.7253</td>\n",
       "      <td>3.2968</td>\n",
       "      <td>3.3047</td>\n",
       "      <td>3.8903</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.85127</td>\n",
       "      <td>0.69896</td>\n",
       "      <td>0.56694</td>\n",
       "      <td>232</td>\n",
       "      <td>231</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.00148</td>\n",
       "      <td>1.230000e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6378</td>\n",
       "      <td>9.5337</td>\n",
       "      <td>75.5998</td>\n",
       "      <td>40.4622</td>\n",
       "      <td>17.8039</td>\n",
       "      <td>10.1614</td>\n",
       "      <td>4.3219</td>\n",
       "      <td>3.2261</td>\n",
       "      <td>6.0101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.76380</td>\n",
       "      <td>0.76449</td>\n",
       "      <td>0.45718</td>\n",
       "      <td>272</td>\n",
       "      <td>271</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>1.420000e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0136</td>\n",
       "      <td>22.4878</td>\n",
       "      <td>6.9231</td>\n",
       "      <td>3.6189</td>\n",
       "      <td>4.0993</td>\n",
       "      <td>5.5304</td>\n",
       "      <td>5.1562</td>\n",
       "      <td>4.6983</td>\n",
       "      <td>3.0422</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.53384</td>\n",
       "      <td>0.74257</td>\n",
       "      <td>0.76349</td>\n",
       "      <td>250</td>\n",
       "      <td>244</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.00739</td>\n",
       "      <td>5.350000e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.2079</td>\n",
       "      <td>7.6563</td>\n",
       "      <td>6.2605</td>\n",
       "      <td>6.0716</td>\n",
       "      <td>7.0607</td>\n",
       "      <td>8.1151</td>\n",
       "      <td>8.7994</td>\n",
       "      <td>8.1001</td>\n",
       "      <td>7.7602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1</td>\n",
       "      <td>0.78411</td>\n",
       "      <td>0.64519</td>\n",
       "      <td>0.46165</td>\n",
       "      <td>193</td>\n",
       "      <td>192</td>\n",
       "      <td>0.009998</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>4.480000e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0645</td>\n",
       "      <td>9.6427</td>\n",
       "      <td>4.5226</td>\n",
       "      <td>7.6517</td>\n",
       "      <td>7.5058</td>\n",
       "      <td>5.1106</td>\n",
       "      <td>4.6773</td>\n",
       "      <td>4.5204</td>\n",
       "      <td>3.6431</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0</td>\n",
       "      <td>0.80259</td>\n",
       "      <td>0.55221</td>\n",
       "      <td>0.20741</td>\n",
       "      <td>602</td>\n",
       "      <td>601</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>6.860000e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>59.7797</td>\n",
       "      <td>33.1091</td>\n",
       "      <td>10.7541</td>\n",
       "      <td>4.2978</td>\n",
       "      <td>3.5306</td>\n",
       "      <td>3.1090</td>\n",
       "      <td>2.7442</td>\n",
       "      <td>3.5607</td>\n",
       "      <td>3.6258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83844</td>\n",
       "      <td>0.56677</td>\n",
       "      <td>0.31278</td>\n",
       "      <td>387</td>\n",
       "      <td>386</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>3.930000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>17.6232</td>\n",
       "      <td>16.9123</td>\n",
       "      <td>14.9890</td>\n",
       "      <td>18.2147</td>\n",
       "      <td>40.3656</td>\n",
       "      <td>66.5518</td>\n",
       "      <td>38.3632</td>\n",
       "      <td>31.1193</td>\n",
       "      <td>9.4326</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>0</td>\n",
       "      <td>0.81319</td>\n",
       "      <td>0.69447</td>\n",
       "      <td>0.42782</td>\n",
       "      <td>346</td>\n",
       "      <td>345</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>4.280000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7181</td>\n",
       "      <td>2.0719</td>\n",
       "      <td>2.4063</td>\n",
       "      <td>3.2230</td>\n",
       "      <td>3.1039</td>\n",
       "      <td>3.0872</td>\n",
       "      <td>6.0785</td>\n",
       "      <td>10.4712</td>\n",
       "      <td>14.0198</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>0</td>\n",
       "      <td>0.65826</td>\n",
       "      <td>0.69723</td>\n",
       "      <td>0.39374</td>\n",
       "      <td>342</td>\n",
       "      <td>341</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.00152</td>\n",
       "      <td>6.800000e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>70.3042</td>\n",
       "      <td>41.9222</td>\n",
       "      <td>22.4267</td>\n",
       "      <td>16.8721</td>\n",
       "      <td>5.6702</td>\n",
       "      <td>4.1347</td>\n",
       "      <td>3.9994</td>\n",
       "      <td>2.8874</td>\n",
       "      <td>2.3640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender      PPE      DFA     RPDE  numPulses  numPeriodsPulses  \\\n",
       "0         0  0.85374  0.76597  0.39811        416               415   \n",
       "1         1  0.83688  0.59464  0.57059        313               312   \n",
       "2         1  0.85127  0.69896  0.56694        232               231   \n",
       "3         1  0.76380  0.76449  0.45718        272               271   \n",
       "4         1  0.53384  0.74257  0.76349        250               244   \n",
       "..      ...      ...      ...      ...        ...               ...   \n",
       "751       1  0.78411  0.64519  0.46165        193               192   \n",
       "752       0  0.80259  0.55221  0.20741        602               601   \n",
       "753       0  0.83844  0.56677  0.31278        387               386   \n",
       "754       0  0.81319  0.69447  0.42782        346               345   \n",
       "755       0  0.65826  0.69723  0.39374        342               341   \n",
       "\n",
       "     meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  ...  \\\n",
       "0            0.004649            0.000084       0.00231  1.070000e-05  ...   \n",
       "1            0.006168            0.000072       0.00171  1.060000e-05  ...   \n",
       "2            0.008314            0.000083       0.00148  1.230000e-05  ...   \n",
       "3            0.007099            0.000094       0.00200  1.420000e-05  ...   \n",
       "4            0.007239            0.001125       0.00739  5.350000e-05  ...   \n",
       "..                ...                 ...           ...           ...  ...   \n",
       "751          0.009998            0.003161       0.00448  4.480000e-05  ...   \n",
       "752          0.003207            0.000017       0.00021  6.860000e-07  ...   \n",
       "753          0.004986            0.000039       0.00079  3.930000e-06  ...   \n",
       "754          0.005589            0.000076       0.00076  4.280000e-06  ...   \n",
       "755          0.004481            0.000124       0.00152  6.800000e-06  ...   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_28  tqwt_kurtosisValue_dec_29  \\\n",
       "0                      10.1883                     6.7917   \n",
       "1                      65.0107                    16.7081   \n",
       "2                       1.6378                     9.5337   \n",
       "3                       2.0136                    22.4878   \n",
       "4                       3.2079                     7.6563   \n",
       "..                         ...                        ...   \n",
       "751                    83.0645                     9.6427   \n",
       "752                    59.7797                    33.1091   \n",
       "753                    17.6232                    16.9123   \n",
       "754                     2.7181                     2.0719   \n",
       "755                    70.3042                    41.9222   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_30  tqwt_kurtosisValue_dec_31  \\\n",
       "0                       3.0096                     2.3981   \n",
       "1                       4.5445                     2.6795   \n",
       "2                      75.5998                    40.4622   \n",
       "3                       6.9231                     3.6189   \n",
       "4                       6.2605                     6.0716   \n",
       "..                         ...                        ...   \n",
       "751                     4.5226                     7.6517   \n",
       "752                    10.7541                     4.2978   \n",
       "753                    14.9890                    18.2147   \n",
       "754                     2.4063                     3.2230   \n",
       "755                    22.4267                    16.8721   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_32  tqwt_kurtosisValue_dec_33  \\\n",
       "0                       2.9229                     4.1730   \n",
       "1                       2.7662                     2.7253   \n",
       "2                      17.8039                    10.1614   \n",
       "3                       4.0993                     5.5304   \n",
       "4                       7.0607                     8.1151   \n",
       "..                         ...                        ...   \n",
       "751                     7.5058                     5.1106   \n",
       "752                     3.5306                     3.1090   \n",
       "753                    40.3656                    66.5518   \n",
       "754                     3.1039                     3.0872   \n",
       "755                     5.6702                     4.1347   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_34  tqwt_kurtosisValue_dec_35  \\\n",
       "0                       4.5809                     2.8216   \n",
       "1                       3.2968                     3.3047   \n",
       "2                       4.3219                     3.2261   \n",
       "3                       5.1562                     4.6983   \n",
       "4                       8.7994                     8.1001   \n",
       "..                         ...                        ...   \n",
       "751                     4.6773                     4.5204   \n",
       "752                     2.7442                     3.5607   \n",
       "753                    38.3632                    31.1193   \n",
       "754                     6.0785                    10.4712   \n",
       "755                     3.9994                     2.8874   \n",
       "\n",
       "     tqwt_kurtosisValue_dec_36  class  \n",
       "0                       2.7978      1  \n",
       "1                       3.8903      1  \n",
       "2                       6.0101      0  \n",
       "3                       3.0422      1  \n",
       "4                       7.7602      1  \n",
       "..                         ...    ...  \n",
       "751                     3.6431      0  \n",
       "752                     3.6258      0  \n",
       "753                     9.4326      0  \n",
       "754                    14.0198      1  \n",
       "755                     2.3640      0  \n",
       "\n",
       "[756 rows x 754 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a34e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting data \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9a42851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df,x = 'class',palette='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d075c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "fig = sns.countplot(x = \"class\" , data = df)\n",
    "plt.xlabel(\"class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Count\")\n",
    "plt.grid(True)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68540dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0279d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'PPE', 'DFA', 'RPDE', 'numPulses', 'numPeriodsPulses',\n",
       "       'meanPeriodPulses', 'stdDevPeriodPulses', 'locPctJitter',\n",
       "       'locAbsJitter',\n",
       "       ...\n",
       "       'tqwt_kurtosisValue_dec_28', 'tqwt_kurtosisValue_dec_29',\n",
       "       'tqwt_kurtosisValue_dec_30', 'tqwt_kurtosisValue_dec_31',\n",
       "       'tqwt_kurtosisValue_dec_32', 'tqwt_kurtosisValue_dec_33',\n",
       "       'tqwt_kurtosisValue_dec_34', 'tqwt_kurtosisValue_dec_35',\n",
       "       'tqwt_kurtosisValue_dec_36', 'class'],\n",
       "      dtype='object', length=754)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02dd1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = df.drop('class',axis=1)\n",
    "dataY = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a3a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling and splitting data to test X and y and train X and y \n",
    "X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.3,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a10dd30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182    1\n",
       "575    0\n",
       "399    1\n",
       "583    1\n",
       "223    0\n",
       "      ..\n",
       "71     1\n",
       "106    1\n",
       "270    1\n",
       "435    1\n",
       "102    1\n",
       "Name: class, Length: 529, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "702702d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2=y_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4320486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad13ec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408    1\n",
       "97     1\n",
       "424    1\n",
       "584    1\n",
       "603    1\n",
       "      ..\n",
       "595    0\n",
       "239    1\n",
       "250    0\n",
       "538    1\n",
       "494    1\n",
       "Name: class, Length: 227, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "909dab2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test2=y_test.tolist()\n",
    "y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "381ae876",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=[]\n",
    "for i in range(len(y_train)):\n",
    "    y_pred1.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a2d4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2=[]\n",
    "for i in range(len(y_test)):\n",
    "    y_pred2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bb83629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ecced17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.34593572778829\n"
     ]
    }
   ],
   "source": [
    "#calculating accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Test accuracy\n",
    "actual = y_train2\n",
    "predicted = y_pred1\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c3841a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.5330396475771\n"
     ]
    }
   ],
   "source": [
    "actual = y_test2\n",
    "predicted = y_pred2\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b83050e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape x: (529, 753)\n",
      "Testing Shape x: (227, 753)\n",
      "\n",
      "\n",
      "Training Shape y: (756, 753)\n",
      "Testing Shape y: (756,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Shape x:',X_train.shape)\n",
    "print(f'Testing Shape x:',X_test.shape)\n",
    "print('\\n')\n",
    "print(f'Training Shape y:',dataX.shape)\n",
    "print(f'Testing Shape y:',dataY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a92829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test= ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d9c9705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74342105 0.74834437 0.74834437 0.74834437 0.74172185]\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.775330396475771\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1]\n",
      "CM: [[  0  51]\n",
      " [  0 176]]\n",
      "Accuracy: 77.5330396475771 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        51\n",
      "           1       0.78      1.00      0.87       176\n",
      "\n",
      "    accuracy                           0.78       227\n",
      "   macro avg       0.39      0.50      0.44       227\n",
      "weighted avg       0.60      0.78      0.68       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying SVC (Support Vector Classification)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Create svm\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "scores = cross_val_score(svm, dataX, dataY, cv=5, scoring='accuracy') \n",
    "print(scores)\n",
    "print(\"Train accuracy:\",svm.score(X_train,y_train))\n",
    "print(\"Test accuracy:\",svm.score(X_test,y_test))\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "print(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'CM:',cm)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred)* 100 ,'%')\n",
    "print(classification_report(y_test, svm.predict(X_test),zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e42fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76315789 0.76821192 0.73509934 0.70198675 0.75496689]\n",
      "Train accuracy: 0.8998109640831758\n",
      "Test accuracy: 0.8898678414096917\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1\n",
      " 0 1 0 1 1]\n",
      "CM: [[ 30  21]\n",
      " [  4 172]]\n",
      "Accuracy: 88.98678414096916 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.59      0.71        51\n",
      "           1       0.89      0.98      0.93       176\n",
      "\n",
      "    accuracy                           0.89       227\n",
      "   macro avg       0.89      0.78      0.82       227\n",
      "weighted avg       0.89      0.89      0.88       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Applying KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# build KNN model and choose n_neighbors = 5\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train, y_train)\n",
    "scores = cross_val_score(knn, dataX, dataY, cv=5, scoring='accuracy') \n",
    "print(scores)\n",
    "print(\"Train accuracy:\",knn.score(X_train,y_train))\n",
    "print(\"Test accuracy:\",knn.score(X_test,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'CM:',cm)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred)* 100 ,'%')\n",
    "print(classification_report(y_test, knn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34a0dbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84210526 0.81456954 0.8013245  0.81456954 0.81456954]\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8105726872246696\n",
      "[0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 0 0 1 0 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1\n",
      " 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1\n",
      " 1 1 0 1 1]\n",
      "CM: [[ 33  18]\n",
      " [ 25 151]]\n",
      "Accuracy: 81.05726872246696 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.65      0.61        51\n",
      "           1       0.89      0.86      0.88       176\n",
      "\n",
      "    accuracy                           0.81       227\n",
      "   macro avg       0.73      0.75      0.74       227\n",
      "weighted avg       0.82      0.81      0.81       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn import tree\n",
    "\n",
    "#Create tree object\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "#Train DT based on scaled training set\n",
    "decision_tree.fit(X_train, y_train)\n",
    "scores = cross_val_score(decision_tree, dataX, dataY, cv=5, scoring='accuracy') \n",
    "print(scores)\n",
    "print(\"Train accuracy:\",decision_tree.score(X_train,y_train))\n",
    "print(\"Test accuracy:\",decision_tree.score(X_test,y_test))\n",
    "\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "print(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'CM:',cm)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred)* 100 ,'%')\n",
    "print(classification_report(y_test, decision_tree.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9dbe319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88157895 0.88741722 0.83443709 0.86092715 0.89403974]\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8986784140969163\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 1]\n",
      "CM: [[ 31  20]\n",
      " [  3 173]]\n",
      "Accuracy: 89.86784140969164 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.61      0.73        51\n",
      "           1       0.90      0.98      0.94       176\n",
      "\n",
      "    accuracy                           0.90       227\n",
      "   macro avg       0.90      0.80      0.83       227\n",
      "weighted avg       0.90      0.90      0.89       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create Random Forest object\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "#Train model\n",
    "random_forest.fit(X_train, y_train)\n",
    "scores = cross_val_score(random_forest, dataX, dataY, cv=5, scoring='accuracy') \n",
    "print(scores)\n",
    "print(\"Train accuracy:\",random_forest.score(X_train,y_train))\n",
    "print(\"Test accuracy:\",random_forest.score(X_test,y_test))\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "print(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'CM:',cm)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred)* 100 ,'%')\n",
    "print(classification_report(y_test, random_forest.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dde6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "#determining transformed features\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "895d8940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74342105 0.74834437 0.74834437 0.74834437 0.74172185]\n",
      "Train accuracy: 0.998109640831758\n",
      "Test accuracy: 0.8105726872246696\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 1 1 1]\n",
      "CM: [[  8  43]\n",
      " [  0 176]]\n",
      "Accuracy: 81.05726872246696 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.16      0.27        51\n",
      "           1       0.80      1.00      0.89       176\n",
      "\n",
      "    accuracy                           0.81       227\n",
      "   macro avg       0.90      0.58      0.58       227\n",
      "weighted avg       0.85      0.81      0.75       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#performing Support Vector Classification (SVC)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Construct SVM\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\n",
    "svm.fit(X_train_pca, y_train)\n",
    "scores = cross_val_score(svm, dataX, dataY, cv=5, scoring='accuracy') \n",
    "print(scores)\n",
    "print(\"Train accuracy:\",svm.score(X_train_pca,y_train))\n",
    "print(\"Test accuracy:\",svm.score(X_test_pca,y_test))\n",
    "\n",
    "y_pred = svm.predict(X_test_pca)\n",
    "print(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'CM:',cm)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred)* 100 ,'%')\n",
    "print(classification_report(y_test, svm.predict(X_test_pca),zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0613b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76315789 0.76821192 0.73509934 0.70198675 0.75496689]\n",
      "Train accuracy: 0.9111531190926276\n",
      "Test accuracy: 0.8634361233480177\n",
      "[1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 0 1 0 1 1]\n",
      "CM: [[ 30  21]\n",
      " [ 10 166]]\n",
      "Accuracy: 86.34361233480176 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.59      0.66        51\n",
      "           1       0.89      0.94      0.91       176\n",
      "\n",
      "    accuracy                           0.86       227\n",
      "   macro avg       0.82      0.77      0.79       227\n",
      "weighted avg       0.86      0.86      0.86       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#constructing KNN model\n",
    "#choosing n_neighbors = 5\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "scores = cross_val_score(knn, dataX, dataY, cv=5, scoring='accuracy') \n",
    "print(scores)\n",
    "print(\"Train accuracy:\",knn.score(X_train_pca,y_train))\n",
    "print(\"Test accuracy:\",knn.score(X_test_pca,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_pca)\n",
    "print(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'CM:',cm)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred)* 100 ,'%')\n",
    "print(classification_report(y_test, knn.predict(X_test_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39300be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85526316 0.82119205 0.81456954 0.81456954 0.82781457]\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.7973568281938326\n",
      "[1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
      " 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 0 1 1]\n",
      "CM: [[ 28  23]\n",
      " [ 23 153]]\n",
      "Accuracy: 79.73568281938326 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55        51\n",
      "           1       0.87      0.87      0.87       176\n",
      "\n",
      "    accuracy                           0.80       227\n",
      "   macro avg       0.71      0.71      0.71       227\n",
      "weighted avg       0.80      0.80      0.80       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn import tree\n",
    "\n",
    "#constructing tree object\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "#Train DT based on scaled training set\n",
    "decision_tree.fit(X_train_pca, y_train)\n",
    "scores = cross_val_score(decision_tree, dataX, dataY, cv=5, scoring='accuracy') \n",
    "print(scores)\n",
    "print(\"Train accuracy:\",decision_tree.score(X_train_pca,y_train))\n",
    "print(\"Test accuracy:\",decision_tree.score(X_test_pca,y_test))\n",
    "\n",
    "y_pred = decision_tree.predict(X_test_pca)\n",
    "print(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'CM:',cm)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred)* 100 ,'%')\n",
    "print(classification_report(y_test, decision_tree.predict(X_test_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9af35bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89473684 0.89403974 0.84768212 0.86092715 0.88741722]\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8722466960352423\n",
      "[1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
      " 1 1 0 1 1]\n",
      "CM: [[ 29  22]\n",
      " [  7 169]]\n",
      "Accuracy: 87.22466960352423 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.57      0.67        51\n",
      "           1       0.88      0.96      0.92       176\n",
      "\n",
      "    accuracy                           0.87       227\n",
      "   macro avg       0.85      0.76      0.79       227\n",
      "weighted avg       0.87      0.87      0.86       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#construct Random Forest object\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "#Train model\n",
    "random_forest.fit(X_train_pca, y_train)\n",
    "scores = cross_val_score(random_forest, dataX, dataY, cv=5, scoring='accuracy') \n",
    "print(scores)\n",
    "print(\"Train accuracy:\",random_forest.score(X_train_pca,y_train))\n",
    "print(\"Test accuracy:\",random_forest.score(X_test_pca,y_test))\n",
    "\n",
    "y_pred = random_forest.predict(X_test_pca)\n",
    "print(y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'CM:',cm)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred)* 100 ,'%')\n",
    "print(classification_report(y_test, random_forest.predict(X_test_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eebdd5",
   "metadata": {},
   "source": [
    "# :نتیجه گیری"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7df59",
   "metadata": {},
   "source": [
    "به جهت افزایش دقت حذف شد id ستون"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a598785",
   "metadata": {},
   "source": [
    " بیشترین دقت را دارد Random Forest  با مقایسه دقت کلی مشاهده می شود که مدل "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99876dba",
   "metadata": {},
   "source": [
    "استفاده از فیجر اکسترکشن تاثیر خوبی روی مدل ها به خصوص مدل ساپورت وکتور ماشین دارد که به مقدار کامپوننت ۱۰ معیارهای ماتریس پیچیدگی به مقدار رضایت بخشی رسید (چون قبل آن برای همه ی داده ها عدد ۱ پیش بینی می شد.)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad051b",
   "metadata": {},
   "source": [
    "تاثیر خوبی گذاشت svm روی دقت مدل PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
